---
title: "624 HW 9"
output:
  pdf_document: default
  html_document: default
---
# Kuhn and Johnson Chapter 8


```{r}
library(mlbench)
library(party)
library(randomForest)
library(ipred)
library(caret)
library(rpart)
library(Cubist)
```


## Problem 8.1
```{r}
set.seed(2000)
simulated <- mlbench.friedman1(200, sd =1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
head(simulated)
```
### a

```{r, echo = FALSE}
model1 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
plot1 <- barplot(rfImp1$Overall, ylab = "importance", xlab = "Variables V1-V1")
plot1
```
As we can see in the above chart, only variables 1-5 significantly effected the model.

### b

```{r, echo = FALSE}
simulated$duplicate1 <- simulated$V1 + rnorm(200) *.1
model2 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp2 <- varImp(model2, scale = FALSE)
plot2 <- barplot(rfImp2$Overall, ylab = "importance", xlab = "Variables V1-V11")
plot2
```
Next, I added an 11th variable that was highly correlated with V1. As we can see from the graph, it is a significant indicator. Additionally, it reduced the importace of V1 because V11 explains some of its variance.

### c
```{r, echo = FALSE}

#simulated$duplicate1 <- NULL
model3 <- cforest(y ~ ., data = simulated)
Imp3 <- varImp(model3)
plot3 <- barplot(Imp3$Overall, ylab = "importance", xlab = "Variables V1-V10")

```

Using conditional trees, we find that fewer predictors are necessary. Namely, variable 3 becomes basically irrelevant.

### d 

```{r}
bag1 <- bagging(y ~ ., data = simulated[, 1:11], nbag = 50)
bag2 <- bagging(y ~ ., data = simulated[, 1:11], nbag = 100)
par(mfrow = c(1,2))
barplot(varImp(bag1)$Overall)
barplot(varImp(bag2)$Overall)
```
Bagging seems to reduce the importance of all the previously important variables and likewise boosting the influence of the previously discarded variables (because explained variance must all equal 1).

```{r, echo = FALSE}

cube1 = cubist(simulated[, 1:10], simulated$y, committees =100)
cube2 = cubist(simulated[, names(simulated) !="y"], y=simulated$y, committees =100 )
imp1 = varImp(cube1)
imp2 = varImp(cube2)
barplot(imp1$Overall)
barplot(imp2$Overall)

```

Cubist tree building, however, gives the results we'd expect from above with the additional benefit of eliminating the importance of the dependent variable (V11).

## Problem 8.2

```{r, echo = FALSE}
x1 <- sample(0:1, 200, replace = TRUE)
x2 <- sample(0:100, 200, replace = TRUE)
x3 <- sample(0:1000, 200, replace = TRUE)
x4 <- x3 + x2 + x1
df <- as.data.frame(cbind(x1,x2,x3,x4))
colnames(df) <- c("Low", "Middle", "High", "Y")
tree <- rpart(Y~., df)
imp <- varImp(tree)
imp
```
We can see that as granularity increases, importance tends to decrease. That is, as the range of our values increases, so does its importance.


## Problem 8.3

### a 

By reducing the bagging fraction, variables with less explanatory power tend to be modelled separately from the more important variables. Conversely, there are fewer opportunities for models to be constructed from the traditionally important vectors. As learning rate increases, the marginal effect of a new tree on the model is increased-- leading to a higher correlation factor. When a learning factor of .1 is used, each additional tree has less effect on the model than a learning factor of .2. This means more trees are needed, but reduces the likelihood of overfitting. That means the relationship bweetn learning rate and the number of trees is inverse.

### b

The model using a learning rate of .9 would have a tendency to overfit as each additional tree has a large marginal effect. 


### c

Interaction depth refers to the tree depth and the number of leaf nodes. As tree depth increases, the number of leaf nodes tends to increase leading to more data vectors coming into play. In both cases, we'd prefer a more uniform distribution of importance. 

## Problem 8.7

### a
```{r}
library(AppliedPredictiveModeling)
library(imputeTS)
data(ChemicalManufacturingProcess)
df <- ChemicalManufacturingProcess
na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
df <- replace(df, TRUE, lapply(df, na_to_mean))
data <- predict(pre, df)
all_indexes = 1:176
training_index = sample(1:176, size = 140)
test_index = setdiff(all_indexes, training_index)
training = data[training_index,]
testing = data[test_index,]
test_labels <- testing['Yield'] 
train_labels <- training['Yield']
train_labels <- lapply(training, as.numeric)
features <- training
features['Yield'] <- NULL
control <- trainControl(method = "cv", number = 10)
```

```{r, echo = FALSE}

# Model 1
# Tuning
grid <- expand.grid(maxdepth = seq(1,10))
model1 <- train(x = features, y = train_labels$Yield, method = "rpart2", metric = "RMSE", tuneGrid = grid, trControl = control)


grid <- expand.grid(mtry=seq(2,38))
model2 <- train(x = features, y = train_labels$Yield, method = "rf", metric = "RMSE", tuneGrid = grid, trControl = control, importance= TRUE)

grid <- expand.grid(committees = c(1,5,10,25,50), neighbors = c(0,1,3,5))
model3 <- train(x = features, y = train_labels$Yield, method = "cubist", metric = "RMSE", trControl = control)




#Predicting

pred1 = predict(model1, testing)
pred2 = predict(model2, testing)
pred3 = predict(model3, testing)

resampling <- resamples(list(SingleTree=model1, RandomForest=model2,  Cubist=model3))

summary(resampling)
```

```{r, echo = FALSE}
rmse1 <- sum(((pred1 - test_labels)^2)^.5)
rmse2 <- sum(((pred2 - test_labels)^2)^.5)
rmse3 <- sum(((pred3 - test_labels)^2)^.5)
barplot(c(rmse1, rmse2, rmse3), names.arg = c("Rpart", "RF", "Cubist"))
```
As we can see, the random forest model performs the best when using RMSE as the indicator, but does not beat the cubist model by much.
### b
```{r}
imp1 <- varImp(model1)
imp2 <- varImp(model2)
imp3 <- varImp(model3)
imp1
imp2
imp3
```

By looking at the above summaries, we can see that Rpart and Cubist models have similar slopes of the importance curve where the random forest model has a much more shallow importance curve. Additionally, Manufacturing processes dominated the models over biological ones.

### c
Below, the optimal tree is described--using a single split of manufacting process 32 around the point .006. Likewise, we can confirm this by looking at the means of the yields of the respective subsets.
```{r, echo = FALSE}
model1$finalModel

trainData <- data.frame(training, train_labels)
less <- subset(training, training$ManufacturingProcess32 < .0063)
more <- subset(training, training$ManufacturingProcess32 >= .0063)
print("Mean of 'less than' yield")
mean(less$Yield)
print("Mean of 'more than' yield")
mean(more$Yield)
```