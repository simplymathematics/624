---
title: "Project 1"
output:
  html_document:
    df_print: paged
---

## Part A

### Reading Data

First, I imported the data using Rstudio and the ```readxl``` package. There are 365 entries for each atm, so I assumed that each is an independent dataset for one of 4 atms starting on May 1st 2009. I have been tasked with forecasting the next 30 days. Please note that unless otherwise noted, the x-axis times are in weeks.
```{r, echo = FALSE}
library(forecast)
library(xlsx)

atm.data <- readxl::read_xlsx("data/ATM624Data.xlsx")
atm1 <- ts(subset(atm.data$Cash, subset = atm.data$ATM == 'ATM1'), frequency = 7)
atm2 <- ts(subset(atm.data$Cash, subset = atm.data$ATM== 'ATM2'), frequency = 7)
atm3 <- ts(subset(atm.data$Cash, subset = atm.data$ATM== 'ATM3'), frequency = 7)
atm4 <- ts(subset(atm.data$Cash, subset = atm.data$ATM== 'ATM4'), frequency = 7)
```
### Cleaning

A single data point from ATM4 was five times the value of any of the others. I assume this is a data entry error. To minimize forecasting error, I replaced this value with the mean of the ATM4 data. This makes the data for ATM4 look like the data for ATMs one and two.

```{r, echo = FALSE}
# Assumer outlier is wrong
par(mfrow = c(1,2))
plot(atm4, main = "With Outlier")
atm4[285] <- mean(atm4)
plot(atm4, main = "Without Outlier")
par(mfrow = c(2,2))
```

Atm 3 was installed recently, so it has less data.
```{r, echo = FALSE}
par(mfrow = c(2,2))
plot(atm1)
plot(atm2)
plot(atm3)
plot(atm4)

```

Due to what I assume is reporting errors, there are several missing data points. To fill in the gaps, I used the STL interpolation function in the ```forecast``` package.
```{r, echo = FALSE}
atm1 <- na.interp(atm1)
atm2 <- na.interp(atm2)
atm3 <- na.interp(atm3)
atm4 <- na.interp(atm4)

par(mfrow = c(2,2))

plot(atm1)
plot(atm2)
plot(atm3)
plot(atm4)


```

The data appears to be noisy, with random variance. However, we have to check for autocorrelation, trends, and seasonality.
```{r, echo = FALSE}
atms <- cbind(atm1, atm2, atm3, atm4)
autoplot(atms)
```

### Preparing Data

In order to perform a forecast on the data, it must be free of integer lags. The ACF plots below show the relationships between data points and the same data various times in the past (week-to-week or month-to-month, for example). These plots merely confirm the presence of lag rather than the size or length.

```{r, echo = FALSE}
par(mfrow = c(2,2))
acf(atm1, lag.max = 30)
acf(atm2)
acf(atm3)
acf(atm4)
```

Luckily, we can use lag plots to see which lag is causing the issues. A plot that more strongly resembles a line corresponds to stronger lag at that number. As we can see across the datasets, lag 7 appears the strongest. Additionally, we can see a diminishing trend in each of the multiples of 7 (as we'd expect). Below are lags 1-30 for each of four atms.

```{r, echo = FALSE}
gglagplot(atm1, lags = 30)
gglagplot(atm2, lags = 30)
gglagplot(atm3, lags = 30)
gglagplot(atm4, lags = 30)
```

This cell shows the output of four different KPSS tests which determine whether or not a time series needs to be differenced in order to be stationary. Only ATM 2 has sifnificant autocorrelation.
```{r, echo = FALSE}
ndiffs(atm1, alpha =.05, test = "kpss")
ndiffs(atm2, alpha =.05, test = "kpss")
remember <- atm2[1]
ndiffs(atm3, alpha =.05, test = "kpss")
ndiffs(atm4, alpha =.05, test = "kpss")

```

After a single differencing, we see that atm2 passes the KPSS test.

```{r, echo = FALSE}
atm2.diff <- diff(atm2, 7)
ndiffs(atm2.diff, alpha=.05, test = 'kpss')
```

### Modelling

The simplest method is simple exponential smoothing. It is appropriate for forecasting data with no clear seasonal pattern or long-term trends. 

```{r, echo = FALSE}

ses1 <- ses(atm1)
ses2 <- ses(atm2.diff)
ses3 <- ses(atm3)
ses4 <- ses(atm4)

par(mfrow = c(2,2))

plot(ses1)
plot(ses2)
plot(ses3)
plot(ses4)

```

The Holt method is like the simple exponential smoothing method above, but includes a trend parameter. As you can see, each ATM is predicted to go up or down, depending on the preceding data.
```{r, echo = FALSE}
holt1 <- holt(atm1, h = 30)
holt2 <- holt(atm2.diff, h = 30)
holt3 <- holt(atm3, h = 30)
holt4 <- holt(atm4, h = 30)
autoplot(atm1) + 
  autolayer(holt1, series = "ATM 1 Projection", PI = FALSE)
autoplot(atm2.diff) + 
  autolayer(holt2, series = "ATM 2 Projection", PI = FALSE)
autoplot(atm3) +
  autolayer(holt3, series = "ATM 3 Projection", PI = FALSE)
autoplot(atm4) + 
  autolayer(holt4, series = "ATM 4 Projection", PI = FALSE) 
```

The Holt-Winters model uses differencing, trends, and seasonality to build a model. It chooses these parameters so as to minimize error.

```{r, echo = FALSE}
hw1 <- hw(atm1);
pred1  <- predict(hw1);
hw2 <- hw(atm2, damped = TRUE, d=1);
pred2  <- predict(hw2);
hw3 <- hw(atm3);
pred3  <- predict(hw3);
hw4 <- hw(atm4);
pred4  <- predict(hw4);

autoplot(atm1) + 
  autolayer(hw1, series = "ATM 1 Projection", PI = FALSE)
autoplot(atm2) + 
  autolayer(hw2, series = "ATM 2 Projection", PI = FALSE)
autoplot(atm3) +
  autolayer(hw3, series = "ATM 3 Projection", PI = FALSE)
autoplot(atm4) + 
  autolayer(hw4, series = "ATM 4 Projection", PI = FALSE) 

```

Finally, there is the arima model, which includes additive and multiplicative effects of data on itself. The ```auto.arima()``` function in R scans through several different models to find the one that minimizes the error between the fitted values and the observed ones.

```{r, echo = FALSE}

arima1 <- auto.arima(atm1);
pred1  <- forecast(atm1, model = arima1, h =31 );
arima2 <- auto.arima(atm2.diff);
pred2  <- forecast(atm2.diff, model = arima2, h =31 );
arima3 <- auto.arima(atm3);
pred3  <- forecast(atm3, model = arima3, h =31 );
arima4 <- auto.arima(atm4);
pred4  <- forecast(atm4, model = arima4, h =31 );

autoplot(atm1) + 
  autolayer(pred1, series = "ATM 1 Projection", PI = FALSE)
autoplot(atm2.diff) + 
  autolayer(pred2, series = "ATM 2 Projection", PI = FALSE)
autoplot(atm3) +
  autolayer(pred3, series = "ATM 3 Projection", PI = FALSE)
autoplot(atm4) + 
  autolayer(pred4, series = "ATM 4 Projection", PI = FALSE) 

```

We can use the root mean square error to compare the results of our models. We square the residuals to amplify the effects of large errors and to remove any negative signs. Then we find the average and the square root to scale it. As we can see below, the Arima model resulted in the smallest RMSE for ATM 1 and a negligible increase for the other atms relative to the other models. However, the Holt-Winters model performed the best consistently.

```{r, echo = FALSE}
first <- c(
mean(ses1$residuals**2)**.5,
mean(holt1$residuals**2)**.5,
mean(hw1$residuals**2)**.5,
mean(arima1$residuals**2)**.5
)
barplot(first, main = "ATM1", names.arg = c("SES", "Holt" , "H-W", "Arima"), ylab = "RMSE")

second <- c(
mean(ses2$residuals**2)**.5,
mean(holt2$residuals**2)**.5,
mean(hw2$residuals**2)**.5,
mean(arima2$residuals**2)**.5
)
barplot(second, main = "ATM2", names.arg = c("SES", "Holt" , "H-W", "Arima"), ylab = "RMSE")

third <- c(
mean(ses3$residuals**2)**.5,
mean(holt3$residuals**2)**.5,
mean(hw3$residuals**2)**.5,
mean(arima3$residuals**2)**.5
)
barplot(third, main = "ATM3", names.arg = c("SES", "Holt" , "H-W", "Arima"), ylab = "RMSE")

fourth <- c(
mean(ses4$residuals**2)**.5,
mean(holt4$residuals**2)**.5,
mean(hw4$residuals**2)**.5,
mean(arima4$residuals**2)**.5
)
barplot(fourth, main = "ATM4", names.arg = c("SES", "Holt" , "H-W", "Arima"), ylab = "RMSE")
```


Finally, we check the residuals to ensure that there is a uniform variance across the model. We see that each ATMs one, two, and four were modelled adequately. For model3, the residuals are larger than expected because this model predicts seasonality that we don't see at this atm(yet). However, due to it success on the other atms, I suggest we keep it here. The larger than testable residuals are expected in this case.

```{r, echo = FALSE}
par(mfrow = c(2,2))
plot(hw1$residuals)
plot(hw2$residuals)
plot(hw3$residuals)
plot(hw4$residuals)

checkresiduals(hw1)
checkresiduals(hw2)
checkresiduals(hw3)
checkresiduals(hw4)
```

Even though our data passed the KPSS test, it is still not white noise. This is either due to the STL model not being the best fit, or the way in which I constructed the initial time series and windowed by week rather than month or quarter. This final model produces residuals with means near 0 and a forecasted variance that corresponds to the observed variance. This upper bounds of this model should give us a great idea of how much cash to have on hand in each atm. The forecasted values are saved in the ```/predictions``` folder. While I included atm3 for the sake of completion, please note that with such a small amount of data, the forecasts are fairly useless.

```{r, echo = FALSE}
rows <- seq(1,31)
rows
final1 <- round(as.data.frame(forecast(atm1, model = hw1, h =31 )),2)
rownames(final1) <- rows
final2 <- round(as.data.frame(forecast(atm2, model = hw2, 31)), 2)
rownames(final2) <- rows
final3 <-round(as.data.frame(forecast(atm3, model = hw3, 31)), 2)
rownames(final3) <- rows
final4 <- round(as.data.frame(forecast(atm4, model = hw4, 31)), 2)
rownames(final4) <- rows

writeMultipleData <- function (file, ...){
    require(xlsx, quietly = TRUE)
    objects <- list(...)
    fargs <- as.list(match.call(expand.dots = TRUE))
    objnames <- c("ATM 1", "ATM 2", "ATM 3", "ATM 4")
    len <- length(objects)
    for (i in 1:len) {
        if (i == 1)
            write.xlsx(objects[[i]], file, sheetName = objnames[i], col.names = TRUE)
        else write.xlsx(objects[[i]], file, sheetName = objnames[i], col.names = TRUE, append = TRUE)
    }
  }

writeMultipleData("predictions/ATMs.xlsx", final1, final2, final3, final4)
```


## Part B

Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward.    Add this to your existing files above. 

```{r, echo = FALSE}
df <- readxl::read_xlsx("data/ResidentialCustomerForecastLoad-624.xlsx")
df$KWH <- as.numeric(df$KWH)
ts <- ts(df$KWH, start = c(1998,1), end = c(2013,12), frequency = 12)
autoplot(ts)
```

Then, I usec STL interpolation to find the missing data. 

```{r, echo = FALSE}

ts <- na.interp(ts)
```

Then, using the ndiff function, we find that we need to difference twice. Additionally, the ACF plot highlights the lag in question (at 1). After differencing, we run another kpss, which the data passes.

```{r, echo = FALSE}
ndiffs(ts, alpha = .05 , test = "kpss")
acf(ts)
ts.diff <- diff(ts,1)
ndiffs(ts.diff, alpha= .05, test = "kpss")
```

```{r, echo = FALSE}
par(mfrow = c(2,2))

model1 <- hw(ts.diff, damped = TRUE)

predictions1 <- predict(model1, 12, prediction.interval=TRUE)
plot(model1)


model2 <- ses(ts.diff)
predictions2 <- predict(model2, 12, prediction.interval = TRUE)
plot(model2)

model3 <- holt(ts.diff)
predictions3 <- predict(model3, 12, prediction.interval = TRUE)
plot(model3)

model4 <- auto.arima(ts, d = 1)

model4 <- auto.arima(ts, d=1);
predictions4 <- predict(ts);
ts.plot(ts, predictions4$pred)

```

We can see below that Holt, Holt-Winters, and SES models all produce residuals that resemble white noise. However, the Auto arima model does not. 
```{r, echo = FALSE}
checkresiduals(model1)
checkresiduals(model2)
checkresiduals(model3)
checkresiduals(model4)
```

The arima model is the best fit for the data. It has the smallest RMSE and its residuals are closest to white noise. 
```{r, echo = FALSE}
accuracy(model1)
accuracy(model2)
accuracy(model3)
accuracy(model4)
```


Then, I saved the file as a .csv in the ```/predictions``` folder under the name partB.csv. We can see below that our expected values and observed values are within the same range and have uniform variance.
```{r, echo = FALSE}
foo <- as.data.frame(ts)
bar <- as.data.frame(predictions1$series)
bar$`Lo 80` <- NULL
bar$`Hi 80` <- NULL
bar$`Lo 95` <- NULL
bar$`Hi 95` <- NULL

tmp <- length(foo)
row.names(bar) = NULL
colnames(bar) = 'x'


i <- dim(foo)[1]

bar <- c(bar$x)
foo <- c(foo$x)

for(index in 1:length(bar)){
  foo[index + i] <- bar[index]
}

plot(foo)
```


```{r, echo = FALSE}
ts.new <- ts(foo, start = c(1998,1), frequency = 12)
partB <- ts.new

ts2csv <- function(x) {
  fname <- paste0("predictions/",deparse(substitute(x)), ".csv")
  readr::write_csv(tsibble::as_tsibble(x, gather = FALSE), fname)
}

ts2csv(partB)

```